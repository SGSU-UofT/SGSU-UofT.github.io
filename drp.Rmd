---
title: "Statistics Directed Reading Program"
---

Welcome! This is the official website of the Statistics Directed Reading Program at UofT. Are you interested in research statistics or eager to explore a statistical topic in more depth? Looking for mentorship and the chance to study material you might not encounter in a course? Then consider applying to our Directed Reading Program (DRP)!

The DRP pairs undergraduate students with graduate mentors in the Department of Statistical Sciences for independent study projects that run through the winter semester. After applying, selected students are matched with mentors based on their interests and goals.

<style>
/* Style the summary (the clickable dropdown header) */
details.type1 summary {
  background-color: #007acc; /* blue */
  color: white;
  padding: 8px;
  border-radius: 5px;
  cursor: pointer;
  font-weight: bold;
}

/* Add hover effect */
details.type1 summary:hover {
  background-color: #365dbf;
}

details.type2 summary {
  background-color: #fce4ec;
  color: #880e4f;
  font-weight: bold;
  padding: 0.4em;
  border-radius: 5px;
  cursor: pointer;
}

details.type2 summary:hover {
  color: #c91019;
  background-color: #e6555c;
}

details.type2 .highlight {
  font-size: 17px;
  color: #880e4f;
  font-weight: bold;
  background: #fff3f3;
  padding: 5px;
  border-left: 3px solid #880e4f;
  margin: 10px 0;
}

/* Style the content inside the dropdown */
details.type1[open] {
  border: 1px solid #007acc;
  padding: 10px;
  margin-top: 5px;
  background-color: #f0f8ff;
  border-radius: 5px;
}

details.type2[open] {
  border: 1px solid #880e4f;
  padding: 10px;
  margin-top: 5px;
  background-color: #fce4ec;
  border-radius: 5px;
}

details.type2 .content {
  font-size: 14px;
  color: #880e4f;
}
</style>


<details class="type1">
<summary>Who is eligible to participate?</summary>
All UofT undergraduate students who are broadly interested in Statistics are eligible. Although students working towards a Statistics major/specialist degree will be prioritized in the application process, we do welcome applicants from mathematics, engineering, computer science etc. with an interest in statistics.
</details>

<details class="type1">
<summary>
What's expected from a DRP student?
</summary>
Students will be paired with a mentor who will guide them through the reading project. The expectation is that students will conduct independent reading and research, having weekly or biweekly meetings with their mentor during the winter semester. At the end of the program, students will submit a short write-up summarizing and explaining what they have learned, with the possibility of giving a short presentation. 
</details>

<details class="type1">
<summary>
Are there any prerequisites required for the SDRP?
</summary>
Although there are no formal prerequisites for the SDRP, different mentors may require proficiency in specific topics and will informally suggest prerequisite classes. It is generally assumed that students are comfortable with linear algebra, basic analysis, probability and estimation theory.
</details>

<details class="type1">
<summary>
Can I choose the topic I want to learn about?
</summary>
Even though the listed topics were chosen specifically as the mentors' specialties, we may consider your own personal proposals if you are motivated and clearly state them in your application.
</details>

[**We are now accepting applications!**](https://forms.office.com/r/7qE19mJZPv)


This DRP is an offshoot of the original program at UChicago's math department and extends a growing [DRP Network](https://sites.google.com/view/drp-network/).


### Projects for the 2026 Session

<details class="type2"><summary>Topics in Optimal Transport</summary>
<div class="content"><p>The transportation problem can be straightforwardly posed as how to optimally move a mass from one area to another. But, this simple question has important applications in economics and, of particular interest to me, generative models in machine learning. My objective to read through the foundations of optimal transport (OT) using the standard textbook "Topics in Optimal Transportation" by Villani, before proceeding (if time permits) to papers that apply this in the context of machine learning.
</p><p class="highlight">Mentor: [KC Tsiolis](https://kctsiolis.github.io/). Suggested prerequisites: strong probability skills and ideally some knowledge of measure theory.</p></div></details>

<details class="type2"><summary>Topics in Concentration Inequalities</summary>
<div class="content"><p>The study of random fluctuations can be made explicit by constructing bounds on the probability that some function differs from its mean by more than a certain amount. If a function of many independent random variables does not depend too much on any of the specific variables then it is concentrated in the sense that, with high probability, it is close to its expected value. This way of thinking has been extremely influential in areas such as statistics, machine learning, learning theory, statistical mechanics or information theory.</p><p class="highlight">Mentor: [Luis Sierra](https://luis-sierra-muntane.github.io/). Suggested prerequisites: strong knowledge of probability.</p></div></details>

<details class="type2"><summary>How is the admissibility of an estimator related to recurrent diffusions?</summary>
<div class="content"><p>Admissibility is a desirable property when constructing an estimator for a random quantity. This paper by Brown shows how questions about admissibility can be translated into questions about diffusions and solving certain boundary value problems from differential equations. Interestingly, some of these mathematical problems turn out to have no solution, and this impossibility reflects situations in statistics where no uniformly best estimator exists. Our goal is to understand how this works and explore deep links between decision theory, probability, and analysis, showing how the structure of random motion can help explain when good estimators do (or don't) exist.</p><p class="highlight">Mentor: [Luis Sierra](https://luis-sierra-muntane.github.io/). Suggested prerequisites: strong analysis skills and some exposure to combinatorics. In particular, knowledge of Markov chains is a big plus.</p></div></details>

<details class="type2"><summary>Causal Effect Identification & Estimation</summary>
<div class="content"><p></p><p class="highlight">Mentor: [Yaqi Shi](https://www.linkedin.com/in/selina-yaqi-shi/?originalSubdomain=ca). Suggested prerequisites: basic linear regression, probability, and STA314 would be preferred.</p></div></details>

<details class="type2"><summary>Topics in Conformal Prediction</summary>
<div class="content"><p>How good are your predictive models? Conformal prediction (CP) is a modern predictive inference tool for high-stakes prediction problems like tumor segmentation and election polling. It has garnered significant interest in the statistics and machine learning communities for its finite sample distribution-free guarantees. In this DRP, we will read the core papers in the area and then branch into advanced subtopics tailored to the student’s interests.</p><p class="highlight">Mentor: [Leo Watson](https://leo-watson.github.io/). Suggested prerequisites: Working knowledge of probability (STA347 equivalent), Bayesian statistics (STA365 equivalent).</p></div></details>

<details class="type2"><summary>Mean-field analysis of neural network training dynamics</summary>
<div class="content"><p>In spite of their tremendous success in practice, neural networks remain in many ways a black box whose evolution over the course of training is poorly understood. One attempt to (partially) remedy this is to study networks in the limit of infinite parameters and infinite training time. These "mean-field dynamics" have given a new perspective on neural network training — one that leverages ideas from the study of interacting particle systems in physics. For some representative papers, see https://arxiv.org/abs/1805.01053, https://arxiv.org/abs/1808.09372, https://arxiv.org/pdf/1804.06561, and https://arxiv.org/pdf/1805.09545.</p><p class="highlight">Mentor: [KC Tsiolis](https://kctsiolis.github.io/). Suggested prerequisites: analysis, probability, and an introductory Machine Learning course (e.g. STA314).</p></div></details>

<details class="type2"><summary>Theory of feature learning in neural networks</summary>
<div class="content"><p>Neural networks set themselves apart from other machine learning methods through their ability to learn useful representations (features) of high-dimensional data. How can we mathematically prove the emergence of these features in neural network training? How can we show a strict separation between those models that learn representations (the "feature learning regime") and to those that don't (the "lazy regime")? Representative papers include https://arxiv.org/pdf/2011.14522, https://arxiv.org/pdf/1812.07956, https://arxiv.org/abs/1906.08899, and https://arxiv.org/pdf/2205.01445.</p><p class="highlight">Mentor: [KC Tsiolis](https://kctsiolis.github.io/). Suggested prerequisites: analysis, probability, and an introductory Machine Learning course (e.g. STA314).</p></div></details>

<details class="type2"><summary>Free Probability to understand random matrices</summary>
<div class="content"><p>Analyzing random matrices has become a fundamental problem in modern statistics. Many complicated properties of large random matrices become simpler when viewed through the lens of “freeness”, a concept that plays a role similar to independence in classical probability. By studying free convolution, limit theorems, and connections to eigenvalue distributions, the project shows how free probability provides clear explanations for phenomena like the semicircle law and universality. We will be exploring free probability as a powerful modern tool for understanding the behavior of large random matrices, offering an accessible pathway from standard probability ideas to advanced techniques used in random matrix theory, operator algebras, and modern mathematical physics.</p><p class="highlight">Mentor: [Luis Sierra](https://luis-sierra-muntane.github.io/). Suggested prerequisites: strong analysis and probability skills and a confident command of linear algebra.</p></div></details>

<details class="type2"><summary>Inverse Problems and Data Assimilation: A Machine Learning Approach</summary>
<div class="content"><p></p><p class="highlight">Mentor: [Yichen Ji](https://user-jiyichen.github.io/). Suggested prerequisites: probability (STA257), linear algebra (MAT223/224), multivariable calculus (MAT237), matrix calculus; basic familiarity with optimization and machine learning concepts is a plus.</p></div></details>

<details class="type2"><summary>Topics in Experimental Design</summary>
<div class="content"><p></p><p class="highlight">Mentor: [Huanlin Mao](https://www.linkedin.com/in/hlmao0919/?originalSubdomain=ca). Suggested prerequisites: analysis, probability, and introductory statistical decision theory.</p></div></details>

<details class="type2"><summary>Statistical Rethinking: A Bayesian Course with Examples in R and Stan</summary>
<div class="content"><p></p><p class="highlight">Mentor: [Yichen Ji](https://user-jiyichen.github.io/). Suggested prerequisites: intro probability (STA237/257), linear regression (STA302); basic Bayesian stats (STA365) or computational stats (STA410) is not required but a plus.</p></div></details>

<details class="type2"><summary>Empirical Bayes: From Herbert Robbins to Modern Theory and Applications</summary>
<div class="content"><p></p><p class="highlight">Mentor: [Yichen Ji](https://user-jiyichen.github.io/). Suggested prerequisites: rigorous probability and statistics (STA347&STA355); mathematical statistics (STA452) is a plus.</p></div></details>

<details class="type2"><summary>Current topics in Actuarial Science</summary>
<div class="content"><p>Students will read about current research areas in actuarial science, which can include theoretical topics such as optimal (re)insurance or applied topics like fraud detection. Depending on the students' background, they will gain some practical experiences in implementation or study potential extensions.</p><p class="highlight">Mentor: [PH Chan](https://ph-chan.github.io/). Suggested prerequisites: (ACT348 or ACT351) with (STA314 or equivalent) or (MAT257 or equivalent) or (STA257 and STA261).</p></div></details>

<details class="type2"><summary>Flow Matching Guide to Code</summary>
<div class="content"><p></p><p class="highlight">Mentor: [Yichen Ji](https://user-jiyichen.github.io/). Suggested prerequisites: ODEs (MAT244), probability (STA257), PyTorch, familiarity with deep learning (STA414/CSC413).</p></div></details>

<details class="type2"><summary>Diffusion Models: A Comprehensive Survey of Methods and Applications</summary>
<div class="content"><p></p><p class="highlight">Mentor: [Yichen Ji](https://user-jiyichen.github.io/). Suggested prerequisites: probability (STA257), multivariate calculus (MAT237), optimization, familiarity with stochastic process and/or deep learning (STA414/CSC413); generative modeling (flows and VAEs) is a plus.</p></div></details>

<details class="type2"><summary>Dimensionality reduction</summary>
<div class="content"><p></p><p class="highlight">Mentor: [Arian Hashemzadeh](https://arian-hashemzadeh.github.io/).</p></div></details>

<details class="type2"><summary>Topics in Causal Inference</summary>
<div class="content"><p></p><p class="highlight">Mentor: [Arian Hashemzadeh](https://arian-hashemzadeh.github.io/).</p></div></details>

<details class="type2"><summary>Statistical Mechanics models for Machine Learning</summary>
<div class="content"><p></p><p class="highlight">Mentor: [Alex Valencia](https://www.linkedin.com/in/alexandervalencia-s/?originalSubdomain=ca). Suggested prerequisites: analysis and probability, with some knowledge of statistical/machine learning</p></div></details>


#### Contact
luis.sierra[at]mail[dot]utoronto[dot]ca

kc.tsiolis[at]mail[dot]utoronto[dot]ca